{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73351a70",
   "metadata": {},
   "source": [
    "### 4000 sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ec5f2177-0ac1-49db-bfde-526d0412c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_42652\\848721970.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'D:\\\\PersonalProjs\\\\osu!\\\\archive\\\\data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['source_id', 'author', 'published_at', 'url_to_image', 'url' ])\n",
    "\n",
    "\n",
    "filtered_df = df[df['source_name'].isin(['GlobeNewswire', 'The Times of India'])]\n",
    "\n",
    "filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "df_relevant_zero = df_cleaned[df_cleaned['relevant'] == 0]\n",
    "df_relevant_one = df_cleaned[df_cleaned['relevant'] == 1]\n",
    "df_sampled = df_relevant_zero.sample(n=1400, random_state=42)\n",
    "balanced_df = pd.concat([df_sampled, df_relevant_one], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af593cdb",
   "metadata": {},
   "source": [
    "### 100k sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6c471369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89541</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89542</td>\n",
       "      <td>Prtimes.jp</td>\n",
       "      <td>RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。</td>\n",
       "      <td>[株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...</td>\n",
       "      <td>RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89543</td>\n",
       "      <td>VOA News</td>\n",
       "      <td>UN Chief Urges World to 'Stop the Madness' of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Kathmandu, Nepal  UN Secretary-General Antonio...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89545</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n",
       "      <td>Ecologists caution against the adverse effects...</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89547</td>\n",
       "      <td>The Times of Israel</td>\n",
       "      <td>200 foreigners, dual nationals cut down in Ham...</td>\n",
       "      <td>France lost 35 citizens, Thailand 33, US 31, U...</td>\n",
       "      <td>Scores of foreign citizens were killed, taken ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105370</th>\n",
       "      <td>781108</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Have done no wrong, only did party work, says ...</td>\n",
       "      <td>The High Court today allowed Shivakumar to wit...</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105371</th>\n",
       "      <td>781129</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>FC Barcelona Guarantees $77.6 Million Champion...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $77.6 mi...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>Home</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105372</th>\n",
       "      <td>781235</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Three hospitals ignored her gravely ill fiancé...</td>\n",
       "      <td>Forty years ago, Sarah Lubarsky came home from...</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>Home</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105373</th>\n",
       "      <td>781240</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>A farmstand in Long Island, Kerber’s Farms has...</td>\n",
       "      <td>Kerbers Farm: Bringing Farm To Table To Manhat...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105374</th>\n",
       "      <td>781308</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Tips For Investing In Short-Term Rentals In Dubai</td>\n",
       "      <td>By exploring your options and keeping a few be...</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105375 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id                   source_name  \\\n",
       "0            89541  International Business Times   \n",
       "1            89542                    Prtimes.jp   \n",
       "2            89543                      VOA News   \n",
       "3            89545            The Indian Express   \n",
       "4            89547           The Times of Israel   \n",
       "...            ...                           ...   \n",
       "105370      781108            The Indian Express   \n",
       "105371      781129                        Forbes   \n",
       "105372      781235                           NPR   \n",
       "105373      781240                        Forbes   \n",
       "105374      781308                        Forbes   \n",
       "\n",
       "                                                    title  \\\n",
       "0       UN Chief Urges World To 'Stop The Madness' Of ...   \n",
       "1                   RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。   \n",
       "2       UN Chief Urges World to 'Stop the Madness' of ...   \n",
       "3       Sikkim warning: Hydroelectricity push must be ...   \n",
       "4       200 foreigners, dual nationals cut down in Ham...   \n",
       "...                                                   ...   \n",
       "105370  Have done no wrong, only did party work, says ...   \n",
       "105371  FC Barcelona Guarantees $77.6 Million Champion...   \n",
       "105372  Three hospitals ignored her gravely ill fiancé...   \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...   \n",
       "105374  Tips For Investing In Short-Term Rentals In Dubai   \n",
       "\n",
       "                                              description  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...   \n",
       "1       [株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...   \n",
       "2       UN Secretary-General Antonio Guterres urged th...   \n",
       "3       Ecologists caution against the adverse effects...   \n",
       "4       France lost 35 citizens, Thailand 33, US 31, U...   \n",
       "...                                                   ...   \n",
       "105370  The High Court today allowed Shivakumar to wit...   \n",
       "105371  FC Barcelona have guaranteed at least $77.6 mi...   \n",
       "105372  Forty years ago, Sarah Lubarsky came home from...   \n",
       "105373  A farmstand in Long Island, Kerber’s Farms has...   \n",
       "105374  By exploring your options and keeping a few be...   \n",
       "\n",
       "                                                  content category  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...    Nepal   \n",
       "1       RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...    Nepal   \n",
       "2       Kathmandu, Nepal  UN Secretary-General Antonio...    Nepal   \n",
       "3       At least 14 persons lost their lives and more ...    Nepal   \n",
       "4       Scores of foreign citizens were killed, taken ...    Nepal   \n",
       "...                                                   ...      ...   \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...     Home   \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...     Home   \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...     Home   \n",
       "105373  Kerbers Farm: Bringing Farm To Table To Manhat...     Home   \n",
       "105374  Cofounder at UpperKey. Passionate about proper...     Home   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "0       UN Secretary-General Antonio Guterres urged th...         0  \n",
       "1                                                     NaN         0  \n",
       "2                                                     NaN         0  \n",
       "3       At least 14 persons lost their lives and more ...         0  \n",
       "4                                                     NaN         0  \n",
       "...                                                   ...       ...  \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...         0  \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...         0  \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...         0  \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...         0  \n",
       "105374  Cofounder at UpperKey. Passionate about proper...         0  \n",
       "\n",
       "[105375 rows x 8 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'D:\\\\PersonalProjs\\\\osu!\\\\archive\\\\data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['source_id', 'author', 'published_at', 'url_to_image', 'url' ])\n",
    "filtered_df = df\n",
    "filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "balanced_df = df_cleaned\n",
    "balanced_df\n",
    "\n",
    "# filtered_df = df[df['source_name'].isin(['GlobeNewswire', 'The Times of India'])]\n",
    "\n",
    "# filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "# df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "# df_relevant_zero = df_cleaned[df_cleaned['relevant'] == 0]\n",
    "# df_relevant_one = df_cleaned[df_cleaned['relevant'] == 1]\n",
    "# df_sampled = df_relevant_zero.sample(n=1400, random_state=42)\n",
    "# balanced_df = pd.concat([df_sampled, df_relevant_one], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bcd90683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Stock          3999\n",
       "Health         2594\n",
       "Finance        2402\n",
       "Technology     2371\n",
       "Real estate    2352\n",
       "               ... \n",
       "Eritrea          14\n",
       "Martinique       13\n",
       "Cabo Verde       11\n",
       "Réunion           9\n",
       "Guadeloupe        4\n",
       "Name: count, Length: 257, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5b4a4744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155465</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Investing Lessons from ICC Cricket World Cup 2023</td>\n",
       "      <td>The factors determining the performance in two...</td>\n",
       "      <td>The scoreboard of the ICC World Cup of 2023 se...</td>\n",
       "      <td>Education</td>\n",
       "      <td>Jimeet Modi CEO, Samco Ventures Modi believes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441303</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>9 reasons why ESG investing is getting popular</td>\n",
       "      <td>ESG is an acronym for Environmental, Social, a...</td>\n",
       "      <td>As we start with Samvat 2080 and winter is aro...</td>\n",
       "      <td>Sustainability</td>\n",
       "      <td>Dr. Poonam Tandon Chief Investment Officer, In...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177431</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Info Edge Q2 Results: Profit rises 24% YoY to ...</td>\n",
       "      <td>Revenue from operations grew 11% to Rs 593 cro...</td>\n",
       "      <td>Info Edge reported 24% growth in its standalon...</td>\n",
       "      <td>Real estate</td>\n",
       "      <td>ETtech Info Edge  reported 24% growth in its s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>646394</td>\n",
       "      <td>GlobeNewswire</td>\n",
       "      <td>Demand for Effective Packaging in the Food Ind...</td>\n",
       "      <td>The United Kingdom stands out as a promising h...</td>\n",
       "      <td>NEWARK, Del, Nov. 23, 2023 (GLOBE NEWSWIRE) --...</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>NEWARK, Del, Nov.  23, 2023  (GLOBE NEWSWIRE) ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>208890</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Inside pics; Khushi Kapoor's birthday celebration</td>\n",
       "      <td>Orhan Awatramani took to his Instagram stories...</td>\n",
       "      <td>Who was Sam Manekshaw, the Indian Army Officer...</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>On November 5, Boney Kapoor and Sridevi’s youn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>757435</td>\n",
       "      <td>GlobeNewswire</td>\n",
       "      <td>BioCryst Announces Approval of ORLADEYO® (bero...</td>\n",
       "      <td>RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...</td>\n",
       "      <td>RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>RESEARCH TRIANGLE PARK, N.C., Nov.  29, 2023  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>757438</td>\n",
       "      <td>GlobeNewswire</td>\n",
       "      <td>Medical Literature Monitoring Services Market ...</td>\n",
       "      <td>Companies covered in this report Clarivate, Te...</td>\n",
       "      <td>Jersey City, NJ, Nov. 29, 2023 (GLOBE NEWSWIRE...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Jersey City, NJ, Nov.  29, 2023  (GLOBE NEWSWI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>757439</td>\n",
       "      <td>GlobeNewswire</td>\n",
       "      <td>Ascendis Pharma Announces Strategic Partnershi...</td>\n",
       "      <td>–   Teijin to receive exclusive license to fur...</td>\n",
       "      <td>Teijin to receive exclusive license to further...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>–   Teijin to receive exclusive license to fur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>757447</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Is there anything left in the market that is u...</td>\n",
       "      <td>“If the spending on elections allows the two-w...</td>\n",
       "      <td>Deepak Shenoy, Founder, Capital Mind, says LIC...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>ETMarkets.com Deepak Shenoy , Founder,  Capita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>757448</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Rate hike cycle didn’t worry Indian market; ho...</td>\n",
       "      <td>“We will have to wait and see how the agricult...</td>\n",
       "      <td>Rushabh Sheth, Co-founder &amp;amp; Co-CIO, Karma ...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>ETMarkets.com Rushabh Sheth , Co-founder &amp; Co-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2056 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id         source_name  \\\n",
       "0         155465  The Times of India   \n",
       "1         441303  The Times of India   \n",
       "2         177431  The Times of India   \n",
       "3         646394       GlobeNewswire   \n",
       "4         208890  The Times of India   \n",
       "...          ...                 ...   \n",
       "2051      757435       GlobeNewswire   \n",
       "2052      757438       GlobeNewswire   \n",
       "2053      757439       GlobeNewswire   \n",
       "2054      757447  The Times of India   \n",
       "2055      757448  The Times of India   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Investing Lessons from ICC Cricket World Cup 2023   \n",
       "1        9 reasons why ESG investing is getting popular   \n",
       "2     Info Edge Q2 Results: Profit rises 24% YoY to ...   \n",
       "3     Demand for Effective Packaging in the Food Ind...   \n",
       "4     Inside pics; Khushi Kapoor's birthday celebration   \n",
       "...                                                 ...   \n",
       "2051  BioCryst Announces Approval of ORLADEYO® (bero...   \n",
       "2052  Medical Literature Monitoring Services Market ...   \n",
       "2053  Ascendis Pharma Announces Strategic Partnershi...   \n",
       "2054  Is there anything left in the market that is u...   \n",
       "2055  Rate hike cycle didn’t worry Indian market; ho...   \n",
       "\n",
       "                                            description  \\\n",
       "0     The factors determining the performance in two...   \n",
       "1     ESG is an acronym for Environmental, Social, a...   \n",
       "2     Revenue from operations grew 11% to Rs 593 cro...   \n",
       "3     The United Kingdom stands out as a promising h...   \n",
       "4     Orhan Awatramani took to his Instagram stories...   \n",
       "...                                                 ...   \n",
       "2051  RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...   \n",
       "2052  Companies covered in this report Clarivate, Te...   \n",
       "2053  –   Teijin to receive exclusive license to fur...   \n",
       "2054  “If the spending on elections allows the two-w...   \n",
       "2055  “We will have to wait and see how the agricult...   \n",
       "\n",
       "                                                content        category  \\\n",
       "0     The scoreboard of the ICC World Cup of 2023 se...       Education   \n",
       "1     As we start with Samvat 2080 and winter is aro...  Sustainability   \n",
       "2     Info Edge reported 24% growth in its standalon...     Real estate   \n",
       "3     NEWARK, Del, Nov. 23, 2023 (GLOBE NEWSWIRE) --...         YouTube   \n",
       "4     Who was Sam Manekshaw, the Indian Army Officer...       Instagram   \n",
       "...                                                 ...             ...   \n",
       "2051  RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...           COVID   \n",
       "2052  Jersey City, NJ, Nov. 29, 2023 (GLOBE NEWSWIRE...           COVID   \n",
       "2053  Teijin to receive exclusive license to further...           COVID   \n",
       "2054  Deepak Shenoy, Founder, Capital Mind, says LIC...           COVID   \n",
       "2055  Rushabh Sheth, Co-founder &amp; Co-CIO, Karma ...           COVID   \n",
       "\n",
       "                                           full_content  relevant  \n",
       "0     Jimeet Modi CEO, Samco Ventures Modi believes ...         0  \n",
       "1     Dr. Poonam Tandon Chief Investment Officer, In...         0  \n",
       "2     ETtech Info Edge  reported 24% growth in its s...         0  \n",
       "3     NEWARK, Del, Nov.  23, 2023  (GLOBE NEWSWIRE) ...         0  \n",
       "4     On November 5, Boney Kapoor and Sridevi’s youn...         0  \n",
       "...                                                 ...       ...  \n",
       "2051  RESEARCH TRIANGLE PARK, N.C., Nov.  29, 2023  ...         1  \n",
       "2052  Jersey City, NJ, Nov.  29, 2023  (GLOBE NEWSWI...         1  \n",
       "2053  –   Teijin to receive exclusive license to fur...         1  \n",
       "2054  ETMarkets.com Deepak Shenoy , Founder,  Capita...         1  \n",
       "2055  ETMarkets.com Rushabh Sheth , Co-founder & Co-...         1  \n",
       "\n",
       "[2056 rows x 8 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbadda7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          source_name  relevant  count\n",
      "0       GlobeNewswire         0    601\n",
      "1       GlobeNewswire         1    448\n",
      "2  The Times of India         0    799\n",
      "3  The Times of India         1    208\n"
     ]
    }
   ],
   "source": [
    "counts = balanced_df.groupby(['source_name', 'relevant']).size().reset_index(name='count')\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "55d74bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = balanced_df['full_content']\n",
    "y = balanced_df['relevant']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3,  random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759d045",
   "metadata": {},
   "source": [
    "### Testing out SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbd7609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = balanced_df['full_content']\n",
    "y = balanced_df['relevant']\n",
    "\n",
    "# Oversampling minority classes using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)\n",
    "print(\"x_smote: \", X_smote)\n",
    "print(\"y_smote: \", y_smote)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Step 4: Stratified Splitting\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.3, stratify=y_smote, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b225574",
   "metadata": {},
   "source": [
    "#### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb27f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       210\n",
      "           1       0.91      0.95      0.93       210\n",
      "\n",
      "    accuracy                           0.93       420\n",
      "   macro avg       0.93      0.93      0.93       420\n",
      "weighted avg       0.93      0.93      0.93       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Encoding Strategies\n",
    "# TF-IDF Vectorizer for classical models\n",
    "X_train_tfidf = X_train\n",
    "X_val_tfidf = X_val\n",
    "X_test_tfidf = X_test\n",
    "\n",
    "# Step 6: Baseline Model (Random Forest)\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b01e15",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "140206da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', num_classes=2):\n",
    "        super(SentenceBertClassifier, self).__init__()\n",
    "        self.model = SentenceTransformer(model_name)  # Pre-trained Sentence-BERT model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.model.get_sentence_embedding_dimension(), num_classes)  # Classification layer\n",
    "\n",
    "    def forward(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_tensor=True)  # Create embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return self.fc(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac16b3",
   "metadata": {},
   "source": [
    "### Testing Various Classifiers, with a) TFIDF Vectorizer b) Univ Sentence Encoder (USE) c) Both\n",
    "1. Linear SVM\n",
    "2. Logistic Regression (LR)\n",
    "3. Naive Bayes\n",
    "4. XGBoost\n",
    "5. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c682e",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c862ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "svm_classifier = LinearSVC()\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "nb_classifier = MultinomialNB()\n",
    "xgboost_classifier = XGBClassifier()\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=3, max_features='sqrt', min_samples_leaf=4, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "def evaluate_model(model, xtest, ytest):\n",
    "    # Make predictions using the model\n",
    "    y_pred = model.predict(xtest)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "    # Calculate precision\n",
    "    precision = precision_score(ytest, y_pred)\n",
    "    # Calculate recall\n",
    "    recall = recall_score(ytest, y_pred)\n",
    "    # Calculate f1 score\n",
    "    f1 = f1_score(ytest, y_pred)\n",
    "    # Generate a classification report and confusion matrix\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    cm = confusion_matrix(ytest, y_pred)\n",
    "    return y_pred, accuracy, precision, recall, f1, report, cm\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501f397",
   "metadata": {},
   "source": [
    "#### 1. TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "22ab86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for embedding TFIDF with train data:-  0.6446653000311926\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "start = timer()\n",
    "tfidf_vectorizer_xtrain = []\n",
    "for i in range(0, len(X_train), batch_size):\n",
    "    batch_X_train = X_train[i:i + batch_size]\n",
    "    batch_X_train_tfidf = tfidf_vectorizer.fit_transform(batch_X_train)\n",
    "    tfidf_vectorizer_xtrain.extend(batch_X_train_tfidf.toarray())\n",
    "print('Required time for embedding TFIDF with train data:- ', timer() - start) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad219ed",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eacf0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for embedding TFIDF with train data:-  0.13175189995672554\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "tfidf_vectorizer_xtest = []\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    batch_X_test = X_test[i:i + batch_size]\n",
    "    batch_X_test_tfidf = tfidf_vectorizer.transform(batch_X_test)\n",
    "    tfidf_vectorizer_xtest.extend(batch_X_test_tfidf.toarray())\n",
    "print('Required time for embedding TFIDF with train data:- ', timer() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8703b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for training Linear SVM classifiers:-  0.0694999000406824\n",
      "Required time for training Logistic Regression classifiers:-  0.11447510001016781\n",
      "Required time for training Naive Bayes classifiers:-  0.03881639998871833\n",
      "Required time for training XGBoost classifiers:-  4.644842999987304\n",
      "Required time for training all five classifiers:-  0.16235200001392514\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "svm_tfidf = svm_classifier.fit(tfidf_vectorizer_xtrain, y_train)\n",
    "print('Required time for training Linear SVM classifiers:- ', timer() - start) \n",
    "start = timer()\n",
    "lr_tfidf = logistic_regression.fit(tfidf_vectorizer_xtrain, y_train)\n",
    "print('Required time for training Logistic Regression classifiers:- ', timer() - start) \n",
    "start = timer()\n",
    "nb_tfidf = nb_classifier.fit(tfidf_vectorizer_xtrain, y_train)\n",
    "print('Required time for training Naive Bayes classifiers:- ', timer() - start) \n",
    "start = timer()\n",
    "xgboost_tfidf = xgboost_classifier.fit(tfidf_vectorizer_xtrain, y_train)\n",
    "print('Required time for training XGBoost classifiers:- ', timer() - start) \n",
    "start = timer()\n",
    "rfc_tfidf = random_forest_classifier.fit(tfidf_vectorizer_xtrain, y_train)\n",
    "print('Required time for training all five classifiers:- ', timer() - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb7470",
   "metadata": {},
   "source": [
    "##### a. Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0feee00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for SVM prediction:-  0.015362299978733063\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "y_pred_svm, accuracy_svm, precision_svm, recall_svm, f1_svm, report_svm, confusion_matrix_svm = evaluate_model(svm_tfidf, tfidf_vectorizer_xtest, y_test)\n",
    "print('Required time for SVM prediction:- ', timer() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f3a9e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVM Accuracy: 0.83\n",
      "LinearSVM Precision: 0.79\n",
      "LinearSVM Recall: 0.73\n",
      "LinearSVM F1 score: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87       198\n",
      "           1       0.79      0.73      0.76       111\n",
      "\n",
      "    accuracy                           0.83       309\n",
      "   macro avg       0.82      0.81      0.82       309\n",
      "weighted avg       0.83      0.83      0.83       309\n",
      "\n",
      "Confusion Matrix\n",
      "[[177  21]\n",
      " [ 30  81]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"LinearSVM Accuracy: {accuracy_svm:.2f}\")\n",
    "print(f\"LinearSVM Precision: {precision_svm:.2f}\")\n",
    "print(f\"LinearSVM Recall: {recall_svm:.2f}\")\n",
    "print(f\"LinearSVM F1 score: {f1_svm:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_svm)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd590f",
   "metadata": {},
   "source": [
    "##### b. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "82ac3d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.80\n",
      "Logistic Regression Precision: 0.77\n",
      "Logistic Regression Recall: 0.61\n",
      "Logistic Regression F1 score: 0.68\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       198\n",
      "           1       0.77      0.61      0.68       111\n",
      "\n",
      "    accuracy                           0.80       309\n",
      "   macro avg       0.79      0.76      0.77       309\n",
      "weighted avg       0.79      0.80      0.79       309\n",
      "\n",
      "Confusion Matrix\n",
      "[[178  20]\n",
      " [ 43  68]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr, accuracy_lr, precision_lr, recall_lr, f1_lr, report_lr, cm_lr = evaluate_model(lr_tfidf, tfidf_vectorizer_xtest, y_test)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")\n",
    "print(f\"Logistic Regression Precision: {precision_lr:.2f}\")\n",
    "print(f\"Logistic Regression Recall: {recall_lr:.2f}\")\n",
    "print(f\"Logistic Regression F1 score: {f1_lr:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_lr)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9b679",
   "metadata": {},
   "source": [
    "##### c. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5d8bc1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes Accuracy: 0.78\n",
      "Naive bayes Precision: 0.71\n",
      "Naive bayes Recall: 0.68\n",
      "Naive bayes F1 score: 0.69\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       198\n",
      "           1       0.71      0.68      0.69       111\n",
      "\n",
      "    accuracy                           0.78       309\n",
      "   macro avg       0.77      0.76      0.76       309\n",
      "weighted avg       0.78      0.78      0.78       309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[167  31]\n",
      " [ 36  75]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb, accuracy_nb, precision_nb, recall_nb, f1_nb, report_nb, cm_nb = evaluate_model(nb_tfidf, tfidf_vectorizer_xtest, y_test)\n",
    "\n",
    "print(f\"Naive bayes Accuracy: {accuracy_nb:.2f}\")\n",
    "print(f\"Naive bayes Precision: {precision_nb:.2f}\")\n",
    "print(f\"Naive bayes Recall: {recall_nb:.2f}\")\n",
    "print(f\"Naive bayes F1 score: {f1_nb:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_nb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1eb5b",
   "metadata": {},
   "source": [
    "##### d. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "38b35cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.90\n",
      "XGBoost Precision: 0.85\n",
      "XGBoost Recall: 0.88\n",
      "XGBoost F1 score: 0.87\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       198\n",
      "           1       0.85      0.88      0.87       111\n",
      "\n",
      "    accuracy                           0.90       309\n",
      "   macro avg       0.89      0.90      0.90       309\n",
      "weighted avg       0.90      0.90      0.90       309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[181  17]\n",
      " [ 13  98]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgboost, accuracy_xgboost, precision_xgboost, recall_xgboost, f1_xgboost, report_xgboost, cm_xgboost = evaluate_model(xgboost_tfidf, tfidf_vectorizer_xtest, y_test)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgboost:.2f}\")\n",
    "print(f\"XGBoost Precision: {precision_xgboost:.2f}\")\n",
    "print(f\"XGBoost Recall: {recall_xgboost:.2f}\")\n",
    "print(f\"XGBoost F1 score: {f1_xgboost:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_xgboost)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea488ad",
   "metadata": {},
   "source": [
    "##### e. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "701c8737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.72\n",
      "Random Forest Precision: 0.88\n",
      "Random Forest Recall: 0.26\n",
      "Random Forest F1 score: 0.40\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.98      0.82       198\n",
      "           1       0.88      0.26      0.40       111\n",
      "\n",
      "    accuracy                           0.72       309\n",
      "   macro avg       0.79      0.62      0.61       309\n",
      "weighted avg       0.77      0.72      0.67       309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[194   4]\n",
      " [ 82  29]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_rfc, accuracy_rfc, precision_rfc, recall_rfc, f1_rfc, report_rfc, cm_rfc = evaluate_model(rfc_tfidf, tfidf_vectorizer_xtest, y_test)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy_rfc:.2f}\")\n",
    "print(f\"Random Forest Precision: {precision_rfc:.2f}\")\n",
    "print(f\"Random Forest Recall: {recall_rfc:.2f}\")\n",
    "print(f\"Random Forest F1 score: {f1_rfc:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_rfc)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f730841b",
   "metadata": {},
   "source": [
    "##### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f92da2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 6.3/8.0 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 29.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 41.4 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7718c46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = ['LinearSVM', 'LogisticRegression', 'NaiveBayes', 'XGBoost', 'RandomForest']\n",
    "model_accuracies = [accuracy_svm, accuracy_lr, accuracy_nb, accuracy_xgboost, accuracy_rfc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, model_accuracies, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Comparison with TFIDF\")\n",
    "plt.ylim(0.0, 1.0)  # Set the y-axis limits\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a295b",
   "metadata": {},
   "source": [
    "#### 2. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = LinearSVC()\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "nb_classifier = MultinomialNB()\n",
    "xgboost_classifier = XGBClassifier()\n",
    "\n",
    "def evaluate_model(model, xtest, ytest):\n",
    "    # Make predictions using the model\n",
    "    y_pred = model.predict(xtest)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "    # Calculate precision\n",
    "    precision = precision_score(ytest, y_pred)\n",
    "    # Calculate recall\n",
    "    recall = recall_score(ytest, y_pred)\n",
    "    # Calculate f1 score\n",
    "    f1 = f1_score(ytest, y_pred)\n",
    "    # Generate a classification report and confusion matrix\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    cm = confusion_matrix(ytest, y_pred)\n",
    "    return y_pred, accuracy, precision, recall, f1, report, cm\n",
    "\n",
    "batch_size = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "95365948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required time for training Linear SVM classifier:-  0.07736399996792898\n",
      "Required time for training Logistic Regression:-  0.02692239999305457\n",
      "Required time for training XGBoost classifier:-  1.6431156999897212\n",
      "Required time for training all Random Forest classifier:-  0.2063522000098601\n",
      "LinearSVM (USE) Accuracy: 0.81\n",
      "LinearSVM (USE) Precision: 0.73\n",
      "LinearSVM (USE) Recall: 0.60\n",
      "LinearSVM (USE) F1 score: 0.66\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       283\n",
      "           1       0.73      0.60      0.66       129\n",
      "\n",
      "    accuracy                           0.81       412\n",
      "   macro avg       0.78      0.75      0.76       412\n",
      "weighted avg       0.80      0.81      0.80       412\n",
      "\n",
      "Confusion Matrix\n",
      "[[255  28]\n",
      " [ 52  77]]\n",
      "Logistic Regression (USE) Accuracy: 0.78\n",
      "Logistic Regression (USE) Precision: 0.72\n",
      "Logistic Regression (USE) Recall: 0.50\n",
      "Logistic Regression (USE) F1 score: 0.59\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       283\n",
      "           1       0.72      0.50      0.59       129\n",
      "\n",
      "    accuracy                           0.78       412\n",
      "   macro avg       0.76      0.70      0.72       412\n",
      "weighted avg       0.77      0.78      0.77       412\n",
      "\n",
      "Confusion Matrix\n",
      "[[258  25]\n",
      " [ 65  64]]\n",
      "XGBoost (USE) Accuracy: 0.82\n",
      "XGBoost (USE) Precision: 0.78\n",
      "XGBoost (USE) Recall: 0.60\n",
      "XGBoost (USE) F1 score: 0.68\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       283\n",
      "           1       0.78      0.60      0.68       129\n",
      "\n",
      "    accuracy                           0.82       412\n",
      "   macro avg       0.81      0.76      0.78       412\n",
      "weighted avg       0.82      0.82      0.82       412\n",
      "\n",
      "Confusion Matrix:\n",
      "[[261  22]\n",
      " [ 51  78]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "X = balanced_df['full_content']\n",
    "y = balanced_df['relevant']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "xtrain_use = []\n",
    "xtest_use = []\n",
    "\n",
    "for i in range(0, len(xtrain), batch_size):\n",
    "    batch_X_train = xtrain[i:i + batch_size]\n",
    "    batch_X_train_use = np.array(embed(batch_X_train))\n",
    "    xtrain_use.extend(batch_X_train_use)\n",
    "\n",
    "for i in range(0, len(xtest), batch_size):\n",
    "    batch_X_test = xtest[i:i + batch_size]\n",
    "    batch_X_test_use = np.array(embed(batch_X_test))\n",
    "    xtest_use.extend(batch_X_test_use)\n",
    "\n",
    "start = timer()\n",
    "svm_use = svm_classifier.fit(xtrain_use, ytrain)\n",
    "print('Required time for training Linear SVM classifier:- ', timer() - start) \n",
    "start = timer()\n",
    "lr_use = logistic_regression.fit(xtrain_use, ytrain)\n",
    "print('Required time for training Logistic Regression:- ', timer() - start)  \n",
    "start = timer()\n",
    "xgboost_use = xgboost_classifier.fit(xtrain_use, ytrain)\n",
    "print('Required time for training XGBoost classifier:- ', timer() - start) \n",
    "start = timer()\n",
    "rfc_use = random_forest_classifier.fit(xtrain_use, ytrain)\n",
    "print('Required time for training all Random Forest classifier:- ', timer() - start) \n",
    "\n",
    "# Linear SVM\n",
    "y_pred_svm_use, accuracy_svm_use, precision_svm_use, recall_svm_use, f1_svm_use, report_svm_use, cm_svm_use = evaluate_model(svm_use, xtest_use, ytest)\n",
    "print(f\"LinearSVM (USE) Accuracy: {accuracy_svm_use:.2f}\")\n",
    "print(f\"LinearSVM (USE) Precision: {precision_svm_use:.2f}\")\n",
    "print(f\"LinearSVM (USE) Recall: {recall_svm_use:.2f}\")\n",
    "print(f\"LinearSVM (USE) F1 score: {f1_svm_use:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_svm_use)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm_svm_use)\n",
    "\n",
    "# Logistic regression\n",
    "y_pred_lr_use, accuracy_lr_use, precision_lr_use, recall_lr_use, f1_lr_use, report_lr_use, cm_lr_use = evaluate_model(lr_use, xtest_use, ytest)\n",
    "print(f\"Logistic Regression (USE) Accuracy: {accuracy_lr_use:.2f}\")\n",
    "print(f\"Logistic Regression (USE) Precision: {precision_lr_use:.2f}\")\n",
    "print(f\"Logistic Regression (USE) Recall: {recall_lr_use:.2f}\")\n",
    "print(f\"Logistic Regression (USE) F1 score: {f1_lr_use:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_lr_use)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm_lr_use)\n",
    "\n",
    "# XGBoost \n",
    "y_pred_xgboost_use, accuracy_xgboost_use, precision_xgboost_use, recall_xgboost_use, f1_xgboost_use, report_xgboost_use, cm_xgboost_use = evaluate_model(xgboost_use, xtest_use, ytest)\n",
    "print(f\"XGBoost (USE) Accuracy: {accuracy_xgboost_use:.2f}\")\n",
    "print(f\"XGBoost (USE) Precision: {precision_xgboost_use:.2f}\")\n",
    "print(f\"XGBoost (USE) Recall: {recall_xgboost_use:.2f}\")\n",
    "print(f\"XGBoost (USE) F1 score: {f1_xgboost_use:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_xgboost_use)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_xgboost_use)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e7223",
   "metadata": {},
   "source": [
    "##### Testing with the highest f1 score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b171e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\ianwa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2b85cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: models_saic\\xgboost_use_model.pkl\n",
      "\n",
      "XGBoost Model Evaluation:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.78\n",
      "Recall: 0.60\n",
      "F1 Score: 0.68\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       283\n",
      "           1       0.78      0.60      0.68       129\n",
      "\n",
      "    accuracy                           0.82       412\n",
      "   macro avg       0.81      0.76      0.78       412\n",
      "weighted avg       0.82      0.82      0.82       412\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[261  22]\n",
      " [ 51  78]]\n",
      "Model loaded successfully.\n",
      "\n",
      "Sample Text: '\n",
      "covid covidcovidcovidcovidcovid covid covid covid'\n",
      "Predicted Label (1 = Relevant, 0 = Irrelevant): 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import joblib  # For saving and loading the model\n",
    "\n",
    "# Define the folder to save the model\n",
    "model_folder = \"models_saic\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "model_path = os.path.join(model_folder, \"xgboost_use_model.pkl\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, xtest, ytest):\n",
    "    y_pred = model.predict(xtest)\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "    precision = precision_score(ytest, y_pred)\n",
    "    recall = recall_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred)\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    cm = confusion_matrix(ytest, y_pred)\n",
    "    return y_pred, accuracy, precision, recall, f1, report, cm\n",
    "\n",
    "# Load Universal Sentence Encoder\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Prepare data\n",
    "X = balanced_df['full_content']\n",
    "y = balanced_df['relevant']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Batch processing for embedding\n",
    "batch_size = 1000\n",
    "xtrain_use = []\n",
    "xtest_use = []\n",
    "\n",
    "for i in range(0, len(xtrain), batch_size):\n",
    "    batch_X_train = xtrain[i:i + batch_size]\n",
    "    batch_X_train_use = np.array(embed(batch_X_train))\n",
    "    xtrain_use.extend(batch_X_train_use)\n",
    "\n",
    "for i in range(0, len(xtest), batch_size):\n",
    "    batch_X_test = xtest[i:i + batch_size]\n",
    "    batch_X_test_use = np.array(embed(batch_X_test))\n",
    "    xtest_use.extend(batch_X_test_use)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "xtrain_use = np.array(xtrain_use)\n",
    "xtest_use = np.array(xtest_use)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "xgboost_classifier = XGBClassifier()\n",
    "xgboost_use = xgboost_classifier.fit(xtrain_use, ytrain)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(xgboost_use, model_path)\n",
    "print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_xgboost_use, accuracy_xgboost_use, precision_xgboost_use, recall_xgboost_use, f1_xgboost_use, report_xgboost_use, cm_xgboost_use = evaluate_model(xgboost_use, xtest_use, ytest)\n",
    "\n",
    "print(f\"\\nXGBoost Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_xgboost_use:.2f}\")\n",
    "print(f\"Precision: {precision_xgboost_use:.2f}\")\n",
    "print(f\"Recall: {recall_xgboost_use:.2f}\")\n",
    "print(f\"F1 Score: {f1_xgboost_use:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", report_xgboost_use)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_xgboost_use)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "86becbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "Sample Text: '\n",
      "The new Covid corruption commissioner has started an investigation into personal protective equipment (PPE) fraud.\n",
      "\n",
      "Tom Hayhoe's first task will be reviewing the £8.7bn of PPE bought during the pandemic that then had to be written off the government's books.\n",
      "\n",
      "Mr Hayhoe is also likely to review the previous government's abandoning of attempts to reclaim money from deals worth £674m.\n",
      "\n",
      "The National Crime Agency is separately investigating possible criminal offences committed in the PPE procurement system.\n",
      "\n",
      "Chris Wormald to become new civil service head\n",
      "Who is new Cabinet Secretary Chris Wormald?\n",
      "UK failed to stockpile crucial protective kit\n",
      "Chancellor Rachel Reeves has asked him to try to recover the public money lost to fraud and underperforming contracts using his experience in procurement as the former chair of an NHS trust.\n",
      "\n",
      "A Treasury source said: \"The chancellor has been clear that she wants this money - that belongs to the British people, and belongs in our public services like our NHS, schools, and police – back.\n",
      "\n",
      "\"She won’t let fraudsters who sought to profit off the back of a national emergency line their pockets.\n",
      "\n",
      "\"Tom Hayhoe brings a wealth of experience and will leave no stone unturned as a commissioner with free rein to investigate the unacceptable carnival of waste and fraud during the pandemic.\"\n",
      "\n",
      "The Department for Health and Social Care (DHSC) lost three-quarters of the £12bn it spent on PPE in the first year of the pandemic, largely due to inflated prices and kit that did not meet requirements.\n",
      "\n",
      "The civil servant who presided over the DHSC during the pandemic, Sir Chris Wormald, has now been appointed to be the UK's most senior civil servant - the Cabinet Secretary.\n",
      "\n",
      "One prominent company that was awarded government PPE contracts worth more than £200 million through a so-called \"VIP lane\" was PPE Medpro, linked to Baroness Michelle Mone.\n",
      "\n",
      "Her husband has since accused the government of trying to \"scapegoat\" the couple for its own failures, instead blaming the DHSC and calling for the resignation of its top civil servant, Sir Chris.\n",
      "\n",
      "Labour had a manifesto commitment to appoint a fixed-term commissioner and use every means possible to recoup public money lost in pandemic-related fraud and from contracts which have not been delivered.\n",
      "\n",
      "Mr Hayhoe's contract is for one year, supported by a small team within the Treasury, and he will report to Reeves directly.\n",
      "\n",
      "He will submit a report at the end of his contract with lessons and recommendations for government procurement in the face of future crises.\n",
      "'\n",
      "Predicted Label (1 = Relevant, 0 = Irrelevant): 0\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model on a sample\n",
    "def test_sample(sample_text, model_path, embed):\n",
    "    # Load the saved model\n",
    "    model = joblib.load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Generate embeddings for the sample text\n",
    "    sample_embedding = np.array(embed([sample_text]))\n",
    "\n",
    "    # Predict relevance\n",
    "    prediction = model.predict(sample_embedding)\n",
    "    return prediction[0]\n",
    "\n",
    "# Test on a sample text\n",
    "sample_text = f\"\"\"\n",
    "The new Covid corruption commissioner has started an investigation into personal protective equipment (PPE) fraud.\n",
    "\n",
    "Tom Hayhoe's first task will be reviewing the £8.7bn of PPE bought during the pandemic that then had to be written off the government's books.\n",
    "\n",
    "Mr Hayhoe is also likely to review the previous government's abandoning of attempts to reclaim money from deals worth £674m.\n",
    "\n",
    "The National Crime Agency is separately investigating possible criminal offences committed in the PPE procurement system.\n",
    "\n",
    "Chris Wormald to become new civil service head\n",
    "Who is new Cabinet Secretary Chris Wormald?\n",
    "UK failed to stockpile crucial protective kit\n",
    "Chancellor Rachel Reeves has asked him to try to recover the public money lost to fraud and underperforming contracts using his experience in procurement as the former chair of an NHS trust.\n",
    "\n",
    "A Treasury source said: \"The chancellor has been clear that she wants this money - that belongs to the British people, and belongs in our public services like our NHS, schools, and police – back.\n",
    "\n",
    "\"She won’t let fraudsters who sought to profit off the back of a national emergency line their pockets.\n",
    "\n",
    "\"Tom Hayhoe brings a wealth of experience and will leave no stone unturned as a commissioner with free rein to investigate the unacceptable carnival of waste and fraud during the pandemic.\"\n",
    "\n",
    "The Department for Health and Social Care (DHSC) lost three-quarters of the £12bn it spent on PPE in the first year of the pandemic, largely due to inflated prices and kit that did not meet requirements.\n",
    "\n",
    "The civil servant who presided over the DHSC during the pandemic, Sir Chris Wormald, has now been appointed to be the UK's most senior civil servant - the Cabinet Secretary.\n",
    "\n",
    "One prominent company that was awarded government PPE contracts worth more than £200 million through a so-called \"VIP lane\" was PPE Medpro, linked to Baroness Michelle Mone.\n",
    "\n",
    "Her husband has since accused the government of trying to \"scapegoat\" the couple for its own failures, instead blaming the DHSC and calling for the resignation of its top civil servant, Sir Chris.\n",
    "\n",
    "Labour had a manifesto commitment to appoint a fixed-term commissioner and use every means possible to recoup public money lost in pandemic-related fraud and from contracts which have not been delivered.\n",
    "\n",
    "Mr Hayhoe's contract is for one year, supported by a small team within the Treasury, and he will report to Reeves directly.\n",
    "\n",
    "He will submit a report at the end of his contract with lessons and recommendations for government procurement in the face of future crises.\n",
    "\"\"\"\n",
    "predicted_label = test_sample(sample_text, model_path, embed)\n",
    "print(f\"\\nSample Text: '{sample_text}'\")\n",
    "print(f\"Predicted Label (1 = Relevant, 0 = Irrelevant): {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c542f6",
   "metadata": {},
   "source": [
    "## Using USE with Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "00412920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 28.5778, Val Loss: 5.7744\n",
      "Epoch [2/10], Train Loss: 25.2132, Val Loss: 5.4191\n",
      "Epoch [3/10], Train Loss: 24.1400, Val Loss: 5.3069\n",
      "Epoch [4/10], Train Loss: 23.5540, Val Loss: 5.2299\n",
      "Epoch [5/10], Train Loss: 23.0092, Val Loss: 5.2592\n",
      "Epoch [6/10], Train Loss: 22.6937, Val Loss: 5.1518\n",
      "Epoch [7/10], Train Loss: 22.3529, Val Loss: 5.1144\n",
      "Epoch [8/10], Train Loss: 22.1672, Val Loss: 5.1010\n",
      "Epoch [9/10], Train Loss: 22.0290, Val Loss: 5.1944\n",
      "Epoch [10/10], Train Loss: 21.9201, Val Loss: 5.0804\n",
      "\n",
      "Test Accuracy: 0.7799352750809061\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84       198\n",
      "           1       0.77      0.55      0.64       111\n",
      "\n",
      "    accuracy                           0.78       309\n",
      "   macro avg       0.78      0.73      0.74       309\n",
      "weighted avg       0.78      0.78      0.77       309\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_42652\\2738385606.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "balanced_df['relevant'] = le.fit_transform(balanced_df['relevant'])\n",
    "\n",
    "# Split the data\n",
    "X = balanced_df['full_content']\n",
    "y = balanced_df['relevant']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Embed Sentences using Universal Sentence Encoder\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "X_train_embeddings = torch.tensor(np.array(embed(X_train.tolist())), dtype=torch.float32)\n",
    "X_val_embeddings = torch.tensor(np.array(embed(X_val.tolist())), dtype=torch.float32)\n",
    "X_test_embeddings = torch.tensor(np.array(embed(X_test.tolist())), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Step 3: Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_embeddings, y_train)\n",
    "val_dataset = TensorDataset(X_val_embeddings, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Define Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_embeddings.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(le.classes_)\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Step 5: Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 6: Train the Model\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "model_folder = \"models_saic\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "model_save_path = os.path.join(model_folder, \"USE_nn_model.pkl\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        embeddings, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            embeddings, labels = batch\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Step 7: Test the Model\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_embeddings)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "    accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e05b6",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5711b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sentence_bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        # self.texts_embeddings = sentence_bert_model.encode(self.texts, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embedding = sentence_bert_model.encode(self.texts[idx], convert_to_tensor=True)\n",
    "\n",
    "        return {\n",
    "            'text': text_embedding,\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    balanced_df['full_content'], balanced_df['relevant'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist())\n",
    "test_dataset = TextDataset(test_texts.tolist(), test_labels.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94503928",
   "metadata": {},
   "source": [
    "### Train and Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c48488e",
   "metadata": {},
   "source": [
    "#### Sentencebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "19004700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agencies China flag Shanghai: A year after historic protests broke out on  Shanghai \\'s bustling Wulumuqi Road, only a subtly increased  police  presence on main junctions betrays anything out of the ordinary. But for many involved in what became  China \\'s most widespread  demonstrations  in decades, it\\'s impossible to erase the memory of the events of last autumn. In the early hours of November 27, 2022, vigils for victims of a fire in Xinjiang\\'s capital Urumqi morphed into multi-city calls to end zero-Covid measures, and even in some cases topple the ruling  Communist Party  and leader  Xi Jinping . Authorities responded by cracking down. But in early December they abruptly lifted the strict health restrictions that had dominated people\\'s lives for almost three years. \"Shortly after zero-Covid lifted, everyone just got back to their normal daily life. Everyone just seems to have moved on, no one\\'s talking about it,\" said Li, a protester in their twenties whose name has been changed for security reasons. For people like Li, there is another reason for the silence: police visited her last month and warned her not to demonstrate. \"When I think about (what happened last year) I still feel I\\'m suffocated by it,\" Li told AFP. Like many, she believed the country\\'s harsh  Covid  rules had hampered rescue efforts when she joined the vigil on Wulumuqi Road to grieve the 10 people killed in the fire. Wulumuqi is the Mandarin name for the city of Urumqi. \"When I saw so many people on that street, although I was mourning, in another way I felt safe,\" she said, recalling the first night of the protests. \"The atmosphere was sad, but also empowering.\" Taking on the regime Protests continued in Shanghai the next day, and ignited in other major cities including  Beijing , Guangzhou and Chengdu, with protesters holding aloft blank sheets of A4 paper to symbolise China\\'s lack of free speech. \"It was not surprising that protests would break out in response to the anti-Covid lockdowns,\" the University of Toronto\\'s Diana Fu told AFP, noting \"bread-and-butter issues\" were common flashpoints in China. \"What was surprising was the blunt anti-regime rhetoric.\" Overt political protest is rare in China, a sophisticated surveillance state that punishes dissent harshly. Li said she had been concerned about freedom of expression before, but \"thought I could live with it, because it didn\\'t affect my everyday life\". Covid changed everything -- especially after being \"trapped... like a prisoner\" in the two-month Shanghai  lockdown . \"People are only going to protest for their rights when it affects them. That\\'s why there were so many people,\" she said. At the time, Durham University\\'s Chenchen Zhang had suggested \"there may not be overarching demand for political reform beyond ending zero-Covid\". Fu noted the protests involved \"a minority of Gen-Z and millennials\" and therefore had not heralded a mass political awakening. For those who did join though, it was \"a watershed moment\", she added. Huang Yicheng, a 27-year-old who was briefly detained on Wulumuqi Road and later fled to Germany, said those who wanted more had \"shouldered a lot of pressure to change the policy of the country\". \"The social movement tide was very big -- but we were stranded like fish on a sand beach\" when people went back to normal after zero-Covid ended, he said. \\'So violent\\' China\\'s security apparatus sprang into action to quash the nascent movement, from scrubbing all online mention of the protests to blanketing cities with officers. On the second night of protests in Shanghai, Li said, police were more prepared to use force. \"They were dragging a girl into a police car -- it was so violent, I keep thinking about that image,\" she said. Huang said he was dragged upside down along the pavement, losing his glasses and shoes.  In the chaos, during which he said he saw numerous women being beaten, he managed to escape without his name being taken. Li was called to a police station a week later, and confronted with a picture of herself at the protest.  \"They asked me to describe what I did and why I was there... in a lot of detail,\" she said.  William Nee, analyst for NGO Chinese Human Rights Defenders, said he estimated more than 100 people had been taken in or detained across the country after the protests. He told AFP he believed most had now been released, except for 19-year-old Uyghur student Kamile Wayit.  Human Rights Watch recently called for her release, along with that of Peng Lifa, who in October 2022 unfurled an anti-government banner across a Beijing bridge.  The Ministry of State Security did not respond to AFP queries about the protests, including about those still detained.  \\'Breaking the norm\\' Huang and Li both attributed zero-Covid\\'s end to the demonstrations, though the extent to which they were responsible for the U-turn is unclear.  \"The longer-term impact is in breaking the norm of protest rhetoric,\" said Fu.  \"Previously, protesters would cloak their demands in economic terms and refrain from directly pointing the finger at Beijing.\" The impact on individuals is more tangible.  Li said some of her friends had left China and planned on never coming back. Huang, too, will not return until he considers it safe. For those who have spoken out, he said, \"we can never go back to normal as before\". Experience Your Economic Times Newspaper, The Digital Way! Monday, 27 Nov, 2023 Read Complete ePaper \\xa0» Digital View Print View Wealth Edition Top Electronic Cos’ Imports Fall in 5 Yrs on Local Play Leading consumer electronic companies such as Apple, Samsung, Dixon Technologies and Havells saw a significant reduction in imports as a percentage of sales in the last fiscal year from the pre-Covid period, according to the latest regulatory data filed by them. PepsiCo India may Name George Kovoor President PepsiCo India is likely to appoint George Kovoor, one of its most senior executives, as president, as the incumbent, Ahmed ElSheikh, is expected to relocate to a global role at the American food and beverage company early next year, said executives with knowledge of the matter. Banks want Boards Kept out of Loan Renewals in ‘Related Party’ Cases  Spotting a brother, the in-laws, or even a stepsister, of a director on a borrower’s board can be maddening for bankers. Still, they can’t disregard the links — lending to companies (where a bank’s director has a ‘connection’) without the clearance or knowledge of the bank’s board is breaking the law. Read More News on china Shanghai covid demonstrations Xi Jinping police communist party lockdown beijing (Catch all the  Business News ,  Breaking News  Events and  Latest News  Updates on  The Economic Times .) Download  The Economic Times News App  to get Daily Market Updates & Live Business News. ... more less Prime Exclusives Investment Ideas Stock Report Plus ePaper Wealth Edition SBI, Bajaj Fin, Axis Bank face the heat of unsecured lending norms. What should investors do next? Tax troubles: Swiggy and Zomato grapple with freshly served GST notice on delivery fee Adam Neumann’s WeWork didn’t work. But India’s flexible workspace startups can do well. Here’s why. Unwinnable wars and the huge toll it takes on economic, humanitarian, and military domains Value investing pips momentum for first time in 15 years. Does it mean the end of trend-following? How Subhiksha\\'s IIM-educated promoter was slapped a 20-year sentence for duping elderly investors 1 2 3 View all Stories'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['full_content'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a732ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "       article_id         source_name  \\\n",
      "0         155465  The Times of India   \n",
      "1         441303  The Times of India   \n",
      "2         177431  The Times of India   \n",
      "3         646394       GlobeNewswire   \n",
      "4         208890  The Times of India   \n",
      "...          ...                 ...   \n",
      "2051      757435       GlobeNewswire   \n",
      "2052      757438       GlobeNewswire   \n",
      "2053      757439       GlobeNewswire   \n",
      "2054      757447  The Times of India   \n",
      "2055      757448  The Times of India   \n",
      "\n",
      "                                                  title  \\\n",
      "0     Investing Lessons from ICC Cricket World Cup 2023   \n",
      "1        9 reasons why ESG investing is getting popular   \n",
      "2     Info Edge Q2 Results: Profit rises 24% YoY to ...   \n",
      "3     Demand for Effective Packaging in the Food Ind...   \n",
      "4     Inside pics; Khushi Kapoor's birthday celebration   \n",
      "...                                                 ...   \n",
      "2051  BioCryst Announces Approval of ORLADEYO® (bero...   \n",
      "2052  Medical Literature Monitoring Services Market ...   \n",
      "2053  Ascendis Pharma Announces Strategic Partnershi...   \n",
      "2054  Is there anything left in the market that is u...   \n",
      "2055  Rate hike cycle didn’t worry Indian market; ho...   \n",
      "\n",
      "                                            description  \\\n",
      "0     The factors determining the performance in two...   \n",
      "1     ESG is an acronym for Environmental, Social, a...   \n",
      "2     Revenue from operations grew 11% to Rs 593 cro...   \n",
      "3     The United Kingdom stands out as a promising h...   \n",
      "4     Orhan Awatramani took to his Instagram stories...   \n",
      "...                                                 ...   \n",
      "2051  RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...   \n",
      "2052  Companies covered in this report Clarivate, Te...   \n",
      "2053  –   Teijin to receive exclusive license to fur...   \n",
      "2054  “If the spending on elections allows the two-w...   \n",
      "2055  “We will have to wait and see how the agricult...   \n",
      "\n",
      "                                                content        category  \\\n",
      "0     The scoreboard of the ICC World Cup of 2023 se...       Education   \n",
      "1     As we start with Samvat 2080 and winter is aro...  Sustainability   \n",
      "2     Info Edge reported 24% growth in its standalon...     Real estate   \n",
      "3     NEWARK, Del, Nov. 23, 2023 (GLOBE NEWSWIRE) --...         YouTube   \n",
      "4     Who was Sam Manekshaw, the Indian Army Officer...       Instagram   \n",
      "...                                                 ...             ...   \n",
      "2051  RESEARCH TRIANGLE PARK, N.C., Nov. 29, 2023 (G...           COVID   \n",
      "2052  Jersey City, NJ, Nov. 29, 2023 (GLOBE NEWSWIRE...           COVID   \n",
      "2053  Teijin to receive exclusive license to further...           COVID   \n",
      "2054  Deepak Shenoy, Founder, Capital Mind, says LIC...           COVID   \n",
      "2055  Rushabh Sheth, Co-founder &amp; Co-CIO, Karma ...           COVID   \n",
      "\n",
      "                                           full_content  relevant  \n",
      "0     Jimeet Modi CEO, Samco Ventures Modi believes ...         0  \n",
      "1     Dr. Poonam Tandon Chief Investment Officer, In...         0  \n",
      "2     ETtech Info Edge  reported 24% growth in its s...         0  \n",
      "3     NEWARK, Del, Nov.  23, 2023  (GLOBE NEWSWIRE) ...         0  \n",
      "4     On November 5, Boney Kapoor and Sridevi’s youn...         0  \n",
      "...                                                 ...       ...  \n",
      "2051  RESEARCH TRIANGLE PARK, N.C., Nov.  29, 2023  ...         1  \n",
      "2052  Jersey City, NJ, Nov.  29, 2023  (GLOBE NEWSWI...         1  \n",
      "2053  –   Teijin to receive exclusive license to fur...         1  \n",
      "2054  ETMarkets.com Deepak Shenoy , Founder,  Capita...         1  \n",
      "2055  ETMarkets.com Rushabh Sheth , Co-founder & Co-...         1  \n",
      "\n",
      "[2056 rows x 8 columns]\n",
      "Epoch [1/10], Loss: 420.3701\n",
      "Epoch [2/10], Loss: 349.8027\n",
      "Epoch [3/10], Loss: 321.4970\n",
      "Epoch [4/10], Loss: 296.3788\n",
      "Epoch [5/10], Loss: 270.3577\n",
      "Epoch [6/10], Loss: 243.7326\n",
      "Epoch [7/10], Loss: 216.9479\n",
      "Epoch [8/10], Loss: 192.0938\n",
      "Epoch [9/10], Loss: 164.4780\n",
      "Epoch [10/10], Loss: 141.3021\n",
      "\n",
      "Accuracy: 0.8106796116504854\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       283\n",
      "           1       0.67      0.77      0.72       129\n",
      "\n",
      "    accuracy                           0.81       412\n",
      "   macro avg       0.78      0.80      0.79       412\n",
      "weighted avg       0.82      0.81      0.81       412\n",
      "\n",
      "Text: The government announces new policies to boost the economy.\n",
      "Prediction: Not Relevant\n",
      "\n",
      "Text: I went hiking in the mountains last weekend.\n",
      "Prediction: Not Relevant\n",
      "\n",
      "Text: China is also the only country that organised experts to share traceability progress with the WHO on many occasions, Mao Ning, spokesperson at the foreign ministry, told a regular news conference. In a statement on Monday, the WHO again asked China to share data and access to assist its efforts to understand the origins of COVID-19, the first cases of which were detected in central China five years ago.\n",
      "Prediction: Not Relevant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "# !pip install pandas torch sentence-transformers scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Create a DataFrame for the Dataset\n",
    "\n",
    "print(\"Dataset:\\n\", balanced_df)\n",
    "\n",
    "# Step 2: Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    balanced_df['full_content'], balanced_df['relevant'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Load SBERT Model and Generate Embeddings\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train_embeddings = torch.tensor(sbert_model.encode(X_train.tolist()))\n",
    "X_test_embeddings = torch.tensor(sbert_model.encode(X_test.tolist()))\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Step 4: Prepare DataLoader\n",
    "train_dataset = TensorDataset(X_train_embeddings, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Step 5: Define Neural Network\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_embeddings.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 2\n",
    "model = SimpleClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Step 6: Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 7: Training Loop with Epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        embeddings, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_embeddings)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# # Step 9: Example Predictions\n",
    "# sample_texts = [\n",
    "#     \"The government announces new policies to boost the economy.\",\n",
    "#     \"I went hiking in the mountains last weekend.\",\n",
    "#     \"China is also the only country that organised experts to share traceability progress with the WHO on many occasions, Mao Ning, spokesperson at the foreign ministry, told a regular news conference. In a statement on Monday, the WHO again asked China to share data and access to assist its efforts to understand the origins of COVID-19, the first cases of which were detected in central China five years ago.\"\n",
    "# ]\n",
    "# sample_embeddings = torch.tensor(sbert_model.encode(sample_texts))\n",
    "# with torch.no_grad():\n",
    "#     sample_outputs = model(sample_embeddings)\n",
    "#     sample_predictions = torch.argmax(sample_outputs, dim=1)\n",
    "\n",
    "# for text, pred in zip(sample_texts, sample_predictions):\n",
    "#     print(f\"Text: {text}\\nPrediction: {'Relevant' if pred == 1 else 'Not Relevant'}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "68973f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_simple_classifier.pth\n",
      "Model loaded for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_42652\\2675711641.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Text: SINGAPORE: The committee which determines the electoral boundaries ahead of each general election has not been formed, the Elections Department (ELD) said on Thursday (Jan 2).\n",
      "\n",
      "“The Electoral Boundaries Review Committee (EBRC) has not been convened,” it said in response to queries from CNA.\n",
      "\n",
      "The committee is convened ahead of every contest to review and make changes to Singapore’s electoral map, taking into account population shifts and housing developments to adjust the number of voters across electoral divisions.\n",
      "\n",
      "The next general election must be held by Nov 23 this year.\n",
      "\n",
      "It will be Singapore’s 14th since independence, and the first under the country’s fourth-generation leadership led by Prime Minister Lawrence Wong.\n",
      "\n",
      "\"STILL SEVERAL MONTHS TO WORK WITH\"\n",
      "National University of Singapore associate professor of political science Chong Ja Ian said that even though the EBRC has not been formed, it does not mean the ruling People's Action Party (PAP) is \"cutting things close\".\n",
      "\n",
      "\"When exactly to hold a GE is at the discretion of the ruling party so long as it is within the timeframe for elections,\" he said, adding that there are \"still several months to work with\".\n",
      "\n",
      "\"In the past, the committee could complete its work in a few weeks. They could probably do so again. A GE can be held shortly after that,\" he said.\n",
      "\n",
      "\"Working backwards, the latest time the EBRC needs to be convened is probably September or October.\"\n",
      "\n",
      "Independent observer Felix Tan, who has written about Singapore’s political landscape, said that Mr Wong \"might want to ensure that the Budget is effectively done and dusted in February before he calls for the EBRC to be formed\".\n",
      "\n",
      "\"If it’s formed in February, one can give them one to three months to churn out a report. This might then put the GE to be held in May or early June,\" he said.\n",
      "\n",
      "Dr Tan said that political parties should not wait until the general election is called before introducing possible candidates.\n",
      "\n",
      "\"After all, a GE will definitely be held this year, so why should there be any hesitation to release their manifesto and candidates earlier?\"\n",
      "\n",
      "Early announcements by parties will also allow Singaporeans to familiarise themselves with the candidates, \"before they become fodder for gossip and derision on social media outlets, which can be unforgiving and toxic to say the least\", added Dr Tan.\n",
      "\n",
      "Related:\n",
      "\n",
      "Govt rejects opposition MPs' electoral boundary suggestions, says EBRC is free from political intervention\n",
      "DURATION OF THE EBRC’S WORK\n",
      "Assoc Prof Chong added that even if the EBRC is convened earlier in the year, it may not have a direct bearing on the exact date of the election. For example, in 2015, the electoral contest took place several months after the release of the EBRC report, he said.\n",
      "\n",
      "In the four contests called by Mr Wong’s predecessor — in 2006, 2011, 2015 and 2020 — the gap from the EBRC’s formation to the election being called ranged between two and almost 11 months.\n",
      "\n",
      "The committee was last convened in August 2019, ahead of a general election in July the following year, held amid the COVID-19 pandemic.\n",
      "\n",
      "In 2015, it was convened in May, with a September election held in what was Singapore’s Golden Jubilee year.\n",
      "\n",
      "Chaired by the secretary to the prime minister, the EBRC comprises senior civil servants from the Housing and Development Board, Singapore Land Authority, Department of Statistics and ELD.\n",
      "\n",
      "The formation of the committee will be a next step in the lead-up to Singapore’s next electoral contest.\n",
      "\n",
      "In March last year, the ELD said about 50,000 public officers would be appointed and trained to manage nomination, polling and counting activities ahead of the next general election.\n",
      "\n",
      "Then Prime Minister Lee Hsien Loong’s May Day Rally last year — his final major speech before handing over the reins — was seen by political analysts as an election campaign speech, as he summed up the PAP government’s achievements during his 20-year tenure.\n",
      "\n",
      "Shortly after taking office, newly minted leader Mr Wong directed the ELD on May 20 to revise Singapore's registers of electors by the end of July.\n",
      "\n",
      "In June, the boundaries of polling districts in 12 constituencies were changed and gazetted.\n",
      "\n",
      "The nation’s voter rolls were then updated and open for public inspection in late July, revealing that over 2.7 million Singaporeans will be eligible to vote in the next general election.\n",
      "Predicted Class: Irrelevant\n",
      "Probability of relevance: 0.2207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = \"trained_simple_classifier.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Function to preprocess and predict on new data\n",
    "def predict_relevance(model, text, sbert_model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predicts the relevance of a given text using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        text: A string containing the input news article.\n",
    "        sbert_model: SentenceTransformer model for encoding.\n",
    "        threshold: Probability threshold for relevance classification.\n",
    "\n",
    "    Returns:\n",
    "        predicted_class: 1 if relevant, 0 if irrelevant.\n",
    "        probability: Probability of relevance.\n",
    "    \"\"\"\n",
    "    # Encode the text using SBERT\n",
    "    encoded_text = torch.tensor(sbert_model.encode([text]))\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_text)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        probability = probabilities[0][1].item()  # Probability of being relevant\n",
    "        predicted_class = int(probability >= threshold)\n",
    "    \n",
    "    return predicted_class, probability\n",
    "\n",
    "# Load the model for testing\n",
    "loaded_model = SimpleClassifier(input_size, hidden_size, num_classes)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded for testing.\")\n",
    "\n",
    "# Load SBERT for encoding new text\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Test on a new sample text\n",
    "sample_text = f\"\"\"SINGAPORE: The committee which determines the electoral boundaries ahead of each general election has not been formed, the Elections Department (ELD) said on Thursday (Jan 2).\n",
    "\n",
    "“The Electoral Boundaries Review Committee (EBRC) has not been convened,” it said in response to queries from CNA.\n",
    "\n",
    "The committee is convened ahead of every contest to review and make changes to Singapore’s electoral map, taking into account population shifts and housing developments to adjust the number of voters across electoral divisions.\n",
    "\n",
    "The next general election must be held by Nov 23 this year.\n",
    "\n",
    "It will be Singapore’s 14th since independence, and the first under the country’s fourth-generation leadership led by Prime Minister Lawrence Wong.\n",
    "\n",
    "\"STILL SEVERAL MONTHS TO WORK WITH\"\n",
    "National University of Singapore associate professor of political science Chong Ja Ian said that even though the EBRC has not been formed, it does not mean the ruling People's Action Party (PAP) is \"cutting things close\".\n",
    "\n",
    "\"When exactly to hold a GE is at the discretion of the ruling party so long as it is within the timeframe for elections,\" he said, adding that there are \"still several months to work with\".\n",
    "\n",
    "\"In the past, the committee could complete its work in a few weeks. They could probably do so again. A GE can be held shortly after that,\" he said.\n",
    "\n",
    "\"Working backwards, the latest time the EBRC needs to be convened is probably September or October.\"\n",
    "\n",
    "Independent observer Felix Tan, who has written about Singapore’s political landscape, said that Mr Wong \"might want to ensure that the Budget is effectively done and dusted in February before he calls for the EBRC to be formed\".\n",
    "\n",
    "\"If it’s formed in February, one can give them one to three months to churn out a report. This might then put the GE to be held in May or early June,\" he said.\n",
    "\n",
    "Dr Tan said that political parties should not wait until the general election is called before introducing possible candidates.\n",
    "\n",
    "\"After all, a GE will definitely be held this year, so why should there be any hesitation to release their manifesto and candidates earlier?\"\n",
    "\n",
    "Early announcements by parties will also allow Singaporeans to familiarise themselves with the candidates, \"before they become fodder for gossip and derision on social media outlets, which can be unforgiving and toxic to say the least\", added Dr Tan.\n",
    "\n",
    "Related:\n",
    "\n",
    "Govt rejects opposition MPs' electoral boundary suggestions, says EBRC is free from political intervention\n",
    "DURATION OF THE EBRC’S WORK\n",
    "Assoc Prof Chong added that even if the EBRC is convened earlier in the year, it may not have a direct bearing on the exact date of the election. For example, in 2015, the electoral contest took place several months after the release of the EBRC report, he said.\n",
    "\n",
    "In the four contests called by Mr Wong’s predecessor — in 2006, 2011, 2015 and 2020 — the gap from the EBRC’s formation to the election being called ranged between two and almost 11 months.\n",
    "\n",
    "The committee was last convened in August 2019, ahead of a general election in July the following year, held amid the COVID-19 pandemic.\n",
    "\n",
    "In 2015, it was convened in May, with a September election held in what was Singapore’s Golden Jubilee year.\n",
    "\n",
    "Chaired by the secretary to the prime minister, the EBRC comprises senior civil servants from the Housing and Development Board, Singapore Land Authority, Department of Statistics and ELD.\n",
    "\n",
    "The formation of the committee will be a next step in the lead-up to Singapore’s next electoral contest.\n",
    "\n",
    "In March last year, the ELD said about 50,000 public officers would be appointed and trained to manage nomination, polling and counting activities ahead of the next general election.\n",
    "\n",
    "Then Prime Minister Lee Hsien Loong’s May Day Rally last year — his final major speech before handing over the reins — was seen by political analysts as an election campaign speech, as he summed up the PAP government’s achievements during his 20-year tenure.\n",
    "\n",
    "Shortly after taking office, newly minted leader Mr Wong directed the ELD on May 20 to revise Singapore's registers of electors by the end of July.\n",
    "\n",
    "In June, the boundaries of polling districts in 12 constituencies were changed and gazetted.\n",
    "\n",
    "The nation’s voter rolls were then updated and open for public inspection in late July, revealing that over 2.7 million Singaporeans will be eligible to vote in the next general election.\"\"\"\n",
    "predicted_class, probability = predict_relevance(loaded_model, sample_text, sbert_model)\n",
    "\n",
    "print(f\"\\nSample Text: {sample_text}\")\n",
    "print(f\"Predicted Class: {'Relevant' if predicted_class == 1 else 'Irrelevant'}\")\n",
    "print(f\"Probability of relevance: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7d100ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEST #####\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "train_examples = [InputExample(texts=[text], label=int(label)) for text, label in zip(pd.Series(tfidf.inverse_transform(X_train.toarray())[0]), y_train)]\n",
    "\n",
    "train_loader = DataLoader(train_examples, shuffle=True, batch_size=16, collate_fn=sentence_bert_model.smart_batching_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4c1274d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1])\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0])\n",
      "tensor([1, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "62acf1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[  101, 16333,   102,     0],\n",
      "        [  101,  7496,   102,     0],\n",
      "        [  101,  5211,   102,     0],\n",
      "        [  101,  5068,   102,     0],\n",
      "        [  101, 12456,  2386,   102],\n",
      "        [  101,  4925,   102,     0],\n",
      "        [  101, 12394,   102,     0],\n",
      "        [  101, 16189,   102,     0],\n",
      "        [  101,  2056,   102,     0],\n",
      "        [  101,  2349,  2015,   102],\n",
      "        [  101, 11683,   102,     0],\n",
      "        [  101,  4610,   102,     0],\n",
      "        [  101,  3143,   102,     0],\n",
      "        [  101,  2861,   102,     0],\n",
      "        [  101, 22472,   102,     0],\n",
      "        [  101,  4248,  4355,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]])}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msentence_bert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Backward pass: compute gradient and update weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:389\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     trans_features \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    391\u001b[0m         trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 5: Encoding Strategies\n",
    "# Using Sentence-BERT Embeddings\n",
    "# sentence_bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_train_embeddings = sentence_bert_model.encode(pd.Series(tfidf.inverse_transform(X_train.toarray())[0]), convert_to_tensor=True)\n",
    "X_val_embeddings = sentence_bert_model.encode(pd.Series(tfidf.inverse_transform(X_val.toarray())[0]), convert_to_tensor=True)\n",
    "X_test_embeddings = sentence_bert_model.encode(pd.Series(tfidf.inverse_transform(X_test.toarray())[0]), convert_to_tensor=True)\n",
    "\n",
    "# # Step 6: DataLoader Preparation\n",
    "# train_examples = [InputExample(texts=[text], label=int(label)) for text, label in zip(pd.Series(tfidf.inverse_transform(X_train.toarray())[0]), y_train)]\n",
    "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16, collate_fn=lambda x: ({'input_ids': sentence_bert_model.tokenize([ex.texts[0] for ex in x])}, torch.tensor([ex.label for ex in x])))\n",
    "\n",
    "# Step 7: Sentence-BERT Training\n",
    "# train_loss = losses.SoftmaxLoss(model=sentence_bert_model, sentence_embedding_dimension=sentence_bert_model.get_sentence_embedding_dimension(), num_labels=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sentence_bert_model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    sentence_bert_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        print(batch[0])\n",
    "        # Extract the input data and labels\n",
    "        sentences = [batch[0][0]['input_ids']]  # List of sentences\n",
    "        labels = batch[1]  # Corresponding labels\n",
    "        \n",
    "        # Compute the sentence embeddings\n",
    "        embeddings = sentence_bert_model.encode(sentences, convert_to_tensor=True)\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing embeddings through the model\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        output = sentence_bert_model(embeddings)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass: compute gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Step 8: Evaluation\n",
    "sentence_bert_model.eval()\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "for text, label in zip(pd.Series(tfidf.inverse_transform(X_val)[0]), y_val):\n",
    "    embeddings = sentence_bert_model.encode(text, convert_to_tensor=True)\n",
    "    prediction = torch.argmax(train_loss(embeddings).detach()).item()\n",
    "    val_predictions.append(prediction)\n",
    "    val_labels.append(label)\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 9: Test Set Evaluation\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "for text, label in zip(pd.Series(tfidf.inverse_transform(X_test)[0]), y_test):\n",
    "    embeddings = sentence_bert_model.encode(text, convert_to_tensor=True)\n",
    "    prediction = torch.argmax(train_loss(embeddings).detach()).item()\n",
    "    test_predictions.append(prediction)\n",
    "    test_labels.append(label)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e13fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        texts = batch['text']\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), torch.mean(torch.tensor(losses))\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            texts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Eval Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), torch.mean(torch.tensor(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed9d39",
   "metadata": {},
   "source": [
    "### GridSearchCV for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92f790ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'dropout': 0.2, 'learning_rate': 1e-05, 'model_name': 'all-MiniLM-L6-v2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "# 1. Define a scikit-learn compatible wrapper for your SentenceBertClassifier\n",
    "class SentenceBertClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', dropout=0.3, learning_rate=2e-5):\n",
    "        self.model_name = model_name\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create the model and optimizer\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = SentenceBertClassifier(\n",
    "            model_name=self.model_name, \n",
    "            num_classes=2\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Convert the input data to DataLoader\n",
    "        train_dataset = TextDataset(X.tolist(), y.tolist())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(5):  # Training for a fixed number of epochs\n",
    "            self.model.train()\n",
    "            for batch in train_loader:\n",
    "                texts = batch['text']\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(texts)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert the input data to DataLoader\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_dataset = TextDataset(X.tolist(), [0] * len(X))  # Dummy labels for prediction\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                texts = batch['text']\n",
    "                outputs = self.model(texts)\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# 2. Define hyperparameters to tune with GridSearchCV\n",
    "param_grid = {\n",
    "    'model_name': ['all-MiniLM-L6-v2', 'paraphrase-MiniLM-L6-v2'],\n",
    "    'dropout': [0.2, 0.3, 0.5],\n",
    "    'learning_rate': [1e-5, 2e-5, 5e-5]\n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=SentenceBertClassifierWrapper(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # You can change this to 'f1' or other metrics if preferred\n",
    "    cv=3,  # Number of cross-validation splits\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use multiple cores for parallel computation\n",
    ")\n",
    "\n",
    "# 4. Fit the GridSearchCV on the training data\n",
    "grid_search.fit(train_texts, train_labels)\n",
    "\n",
    "# 5. Get the best parameters and evaluate\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db57c22",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8789220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     14\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[60], line 13\u001b[0m, in \u001b[0;36mSentenceBertClassifier.forward\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[1;32m---> 13\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Create embeddings\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:586\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    585\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 586\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[0;32m    587\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [sentences[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1468\u001b[0m, in \u001b[0;36mSentenceTransformer._text_length\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text)\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1087\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen() of a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1089\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1096\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of a 0-d tensor"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "# Initialize Model, Criterion, and Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentenceBertClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Training\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    eval_acc, eval_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Evaluation Accuracy: {eval_acc:.4f}, Evaluation Loss: {eval_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
